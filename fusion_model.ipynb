{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff78576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob \n",
    "import math\n",
    "\n",
    "import torch # For building the networks \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from nystrom_attention import NystromAttention\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "from lifelines.utils import concordance_index\n",
    "# import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# from TCGA_preprocess import TCGA_csv_process\\\n",
    "import sys\n",
    "\n",
    "#Feature extraction\n",
    "# from utils import perform_clustering, select_representative_images, extract_features\n",
    "# import torchvision.models as models\n",
    "import openslide as op\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def neg_par_log_likelihood(pred,ytime,yevent):\n",
    "    n_observed = yevent.sum(0)\n",
    "    if(n_observed==0):\n",
    "        return 0.0\n",
    "    \n",
    "    ytime_indicator = R_set(ytime).float()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        ytime_indicator = ytime_indicator.cuda()\n",
    "    # pred = torch.tensor(pred.values).cuda()\n",
    "    \n",
    "    risk_set_sum = ytime_indicator.mm(torch.exp(pred.float()))\n",
    "    \n",
    "    diff = pred - torch.log(risk_set_sum)\n",
    "    \n",
    "    yevent = yevent[:,None]\n",
    "    \n",
    "    sum_diff_in_observed = torch.transpose(diff,0,1).mm(yevent.float())\n",
    "    \n",
    "    cost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))\n",
    "    \n",
    "    return cost\n",
    "#https://github.com/tomcat123a/survival_loss_criteria/blob/master/loss.py\n",
    "def R_set(x):\n",
    "\t'''Create an indicator matrix of risk sets, where T_j >= T_i.\n",
    "\tNote that the input data have been sorted in descending order.\n",
    "\tInput:\n",
    "\t\tx: a PyTorch tensor that the number of rows is equal to the number of samples.\n",
    "\tOutput:\n",
    "\t\tindicator_matrix: an indicator matrix (which is a lower traiangular portions of matrix).\n",
    "\t'''\n",
    "  \n",
    "\tn_sample = x.size(0)\n",
    "\tmatrix_ones = torch.ones(n_sample, n_sample)\n",
    "\tindicator_matrix = torch.triu(matrix_ones)\n",
    "    \n",
    "\n",
    "\treturn(indicator_matrix)\n",
    "    \n",
    "# Define your custom dataset class\n",
    "class SurvivalDataset(Dataset):\n",
    "    def __init__(self, feat_paths, surv_data, clinical_data, num_patches = 10000, transform=None, mtlr = True):\n",
    "        '''\n",
    "        surv_data : ('TCGA-44-6777', 987.0, 1.0) [patient id, OS.time, OS]\n",
    "\n",
    "        num_patch : int (>=50)\n",
    "        '''\n",
    "        self.feat_paths = feat_paths\n",
    "        self.survival_data = surv_data\n",
    "        self.clinical_data = clinical_data\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.mtlr = mtlr\n",
    "        # self.cancer_classifier = cancer_classifier\n",
    "    def __len__(self):\n",
    "        return len(self.feat_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #max_num_patches = 144501\n",
    "        desired_shape = (10000, 512)\n",
    "       \n",
    "        patches = torch.load(self.feat_paths[idx])\n",
    "     \n",
    "        if patches.shape[0] < desired_shape[0]:\n",
    "            # If the tensor has more rows than the desired shape, pad it with zeros\n",
    "            padding_needed = desired_shape[0] - patches.shape[0]\n",
    "            patches = torch.nn.functional.pad(patches, (0, 0, 0, padding_needed))\n",
    "        else:\n",
    "            # Shuffle the indices\n",
    "            shuffled_indices = torch.randperm(patches.size(0))\n",
    "            \n",
    "            # Use the shuffled indices to rearrange the rows of the tensor\n",
    "            patches = patches[shuffled_indices][:desired_shape[0]]\n",
    "                \n",
    "        event = self.survival_data[idx][3]\n",
    "        time = self.survival_data[idx][1]\n",
    "        \n",
    "        if self.mtlr:\n",
    "            time_val = self.survival_data[idx][2]\n",
    "            time = np.array(time, dtype=np.float32)\n",
    "            time = torch.tensor(time, dtype=torch.float32)\n",
    "            \n",
    "         # Convert clinical_data to a numpy array with float type\n",
    "        clinical_data = np.array(self.clinical_data[idx], dtype=np.float32)\n",
    "        \n",
    "        # Convert the numpy array to a PyTorch tensor\n",
    "        clinical_data = torch.tensor(clinical_data, dtype=torch.float32)\n",
    "        \n",
    "        return patches, time, event, clinical_data, time_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7061d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lifelines\n",
    "# !pip install nystrom-attention\n",
    "# !pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d70153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8eb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_layer=nn.LayerNorm, dim=512):\n",
    "        super().__init__()\n",
    "        self.norm = norm_layer(dim)\n",
    "        self.attn = NystromAttention(\n",
    "            dim = dim,\n",
    "            dim_head = dim//8, #8\n",
    "            heads = 2, #8\n",
    "            num_landmarks = dim//2,    # number of landmarks\n",
    "            pinv_iterations = 6,    # number of moore-penrose iterations for approximating pinverse. 6 was recommended by the paper\n",
    "            residual = True,         # whether to do an extra residual with the value or not. supposedly faster convergence if turned on\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attn(self.norm(x))\n",
    "        x= x + a\n",
    "\n",
    "        return x, None\n",
    "\n",
    "\n",
    "class PPEG(nn.Module):\n",
    "    def __init__(self, dim=512):\n",
    "        super(PPEG, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, 7, 1, 7//2, groups=dim)\n",
    "        self.proj1 = nn.Conv2d(dim, dim, 5, 1, 5//2, groups=dim)\n",
    "        self.proj2 = nn.Conv2d(dim, dim, 3, 1, 3//2, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, _, C = x.shape\n",
    "        cls_token, feat_token = x[:, 0], x[:, 1:]\n",
    "        cnn_feat = feat_token.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.proj(cnn_feat)+cnn_feat+self.proj1(cnn_feat)+self.proj2(cnn_feat)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = torch.cat((cls_token.unsqueeze(1), x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8df1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchsurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d10b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn-pandas\n",
    "from torchsurv.loss import cox\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "def CoxLoss(estimate, event, time):\n",
    "    \"\"\"\n",
    "    estimate : (B, 1)\n",
    "    event : (B, 1)\n",
    "    time : (B, 1)\n",
    "    B : Batch size\n",
    "    \"\"\"\n",
    "    loss = cox.neg_partial_log_likelihood(estimate, event.bool(), time)\n",
    "    return loss\n",
    "\n",
    "def Cindex(estimate, event, time):\n",
    "    cindex = ConcordanceIndex()\n",
    "    c = cindex(estimate, event.bool(), time)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e851741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "# # input should be a distribution in the log space\n",
    "# input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "# # Sample a batch of distributions. Usually this would come from the dataset\n",
    "# target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "# output = kl_loss(input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203528c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLR(nn.Module):\n",
    "    \"\"\"Multi-task logistic regression for individualised\n",
    "    survival prediction.\n",
    "    \n",
    "    The MTLR time-logits are computed as:\n",
    "    `z = sum_k x^T w_k + b_k`,\n",
    "    where `w_k` and `b_k` are learnable weights and biases for each time interval.\n",
    "    \n",
    "    Note that a slightly more efficient reformulation is used here, first proposed\n",
    "    in [2]_.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    ..[1] C.-N. Yu et al., â€˜Learning patient-specific cancer survival distributions \n",
    "    as a sequence of dependent regressorsâ€™, in Advances in neural information processing systems 24,\n",
    "    2011, pp. 1845â€“1853.\n",
    "    ..[2] P. Jin, â€˜Using Survival Prediction Techniques to Learn Consumer-Specific Reservation Price Distributionsâ€™,\n",
    "    Master's thesis, University of Alberta, Edmonton, AB, 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, num_time_bins):\n",
    "        \"\"\"Initialises the module.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Number of input features.\n",
    "        num_time_bins : int\n",
    "            The number of bins to divide the time axis into.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_time_bins = num_time_bins\n",
    "\n",
    "        weight = torch.zeros(self.in_features,\n",
    "                             self.num_time_bins-1,\n",
    "                             dtype=torch.float)\n",
    "        bias = torch.zeros(self.num_time_bins-1)\n",
    "        self.mtlr_weight = nn.Parameter(weight)\n",
    "        self.mtlr_bias = nn.Parameter(bias)\n",
    "        \n",
    "        # `G` is the coding matrix from [2]_ used for fast summation.\n",
    "        # When registered as buffer, it will be automatically\n",
    "        # moved to the correct device and stored in saved\n",
    "        # model state.\n",
    "        self.register_buffer(\"G\", \n",
    "                             torch.tril(torch.ones(self.num_time_bins-1, \n",
    "                                                   self.num_time_bins, requires_grad=True)))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on a batch of examples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, shape (num_samples, num_features)\n",
    "            The input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape (num_samples, num_time_bins)\n",
    "            The predicted time logits.\n",
    "        \"\"\"\n",
    "        out = torch.matmul(x, self.mtlr_weight) + self.mtlr_bias\n",
    "        return torch.matmul(out, self.G)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets the model parameters.\"\"\"\n",
    "        nn.init.xavier_normal_(self.mtlr_weight)\n",
    "        nn.init.constant_(self.mtlr_bias, 0.)\n",
    "\n",
    "\n",
    "def masked_logsumexp(x, mask, dim=-1):\n",
    "    \"\"\"Computes logsumexp over elements of a tensor specified by a mask in a numerically stable way.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        The input tensor.\n",
    "    mask : torch.Tensor\n",
    "        A tensor with the same shape as `x` with 1s in positions that should\n",
    "        be used for logsumexp computation and 0s everywhere else.\n",
    "    dim : int\n",
    "        The dimension of `x` over which logsumexp is computed. Default -1 uses\n",
    "        the last dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Tensor containing the logsumexp of each row of `x` over `dim`.\n",
    "    \"\"\"\n",
    "    max_val, _ = (x * mask).max(dim=dim)\n",
    "    max_val = torch.clamp_min(max_val, 0)\n",
    "    return torch.log(torch.sum(torch.exp(x - max_val.unsqueeze(dim)) * mask, dim=dim)) + max_val\n",
    "\n",
    "\n",
    "def mtlr_neg_log_likelihood(logits, target, average=False):\n",
    "    \"\"\"Computes the negative log-likelihood of a batch of model predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor, shape (num_samples, num_time_bins)\n",
    "        Tensor with the time-logits (as returned by the MTLR module) for one instance\n",
    "        in each row.\n",
    "    target : torch.Tensor, shape (num_samples, num_time_bins)\n",
    "        Tensor with the encoded ground truth survival.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The predicted survival curves for each row in `pred` at timepoints used during training.\n",
    "    \"\"\"\n",
    "    censored = target.sum(dim=1) > 1\n",
    " \n",
    "    if censored.any():\n",
    "        nll_censored = masked_logsumexp(logits[censored], target[censored]).sum()\n",
    "    else:\n",
    "        nll_censored = 0.\n",
    "    if (~censored).any():\n",
    "        nll_uncensored = (logits[~censored] * target[~censored]).sum()\n",
    "    else:\n",
    "        nll_uncensored = 0.\n",
    "    \n",
    "    # the normalising constant\n",
    "    norm = torch.logsumexp(logits, dim=1).sum()\n",
    "    nll_total = -(nll_censored + nll_uncensored - norm)\n",
    "    if average:\n",
    "        nll_total = nll_total / target.size(0)\n",
    "    \n",
    "    return nll_total\n",
    "\n",
    "\n",
    "def mtlr_survival(logits):\n",
    "    \"\"\"Generates predicted survival curves from predicted logits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        Tensor with the time-logits (as returned by the MTLR module) for one instance\n",
    "        in each row.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The predicted survival curves for each row in `pred` at timepoints used during training.\n",
    "    \"\"\"\n",
    "    # TODO: do not reallocate G in every call\n",
    "    G = torch.tril(torch.ones(logits.size(1), logits.size(1))).to(logits.device)\n",
    "    density = torch.softmax(logits, dim=1)\n",
    "    return torch.matmul(density, G)\n",
    "\n",
    "\n",
    "def mtlr_survival_at_times(logits, train_times, pred_times):\n",
    "    \"\"\"Generates predicted survival curves at arbitrary timepoints using linear interpolation.\n",
    "    \n",
    "    This function uses scipy.interpolate internally and returns a Numpy array, in contrast\n",
    "    with `mtlr_survival`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor \n",
    "        Tensor with the time-logits (as returned by the MTLR module) for one instance\n",
    "        in each row.\n",
    "    train_times : Tensor or ndarray\n",
    "        Time bins used for model training. Must have the same length as the first dimension\n",
    "        of `pred`.\n",
    "    pred_times : np.ndarray\n",
    "        Array of times used to compute the survival curve.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The survival curve for each row in `pred` at `pred_times`. The values are linearly interpolated\n",
    "        at timepoints not used for training.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        surv = mtlr_survival(logits).cpu().numpy()\n",
    "    interpolator = interp1d(train_times, surv)\n",
    "    return interpolator(pred_times)\n",
    "\n",
    "\n",
    "def mtlr_hazard(logits):\n",
    "    \"\"\"Computes the hazard function from MTLR predictions.\n",
    "    \n",
    "    The hazard function is the instantenous rate of failure, i.e. roughly\n",
    "    the risk of event at each time interval. It's computed using\n",
    "    `h(t) = f(t) / S(t)`,\n",
    "    where `f(t)` and `S(t)` are the density and survival functions at t, respectively.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        The predicted logits as returned by the `MTLR` module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The hazard function at each time interval in `y_pred`.\n",
    "    \"\"\"\n",
    "    return torch.softmax(logits, dim=1)[:, :-1] / (mtlr_survival(logits) + 1e-15)[:, 1:]\n",
    "\n",
    "\n",
    "def mtlr_risk(logits):\n",
    "    \"\"\"Computes the overall risk of event from MTLR predictions.\n",
    "    \n",
    "    The risk is computed as the time integral of the cumulative hazard,\n",
    "    as defined in [1]_.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : torch.Tensor\n",
    "        The predicted logits as returned by the `MTLR` module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The predicted overall risk.\n",
    "    \"\"\"\n",
    "    hazard = mtlr_hazard(logits)\n",
    "    return torch.sum(hazard.cumsum(1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ededb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def make_time_bins(times, num_bins=None, use_quantiles=True):\n",
    "    \"\"\"Creates the bins for survival time discretisation.\n",
    "    \n",
    "    By default, sqrt(num_observation) bins corresponding to the quantiles of \n",
    "    the survival time distribution are used, as in https://github.com/haiderstats/MTLR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    times : np.ndarray\n",
    "        Array of survival times.\n",
    "    num_bins : int, optional\n",
    "        The number of bins to use. If None (default), sqrt(num_observations)\n",
    "        bins will be used.\n",
    "    use_quantiles : bool\n",
    "        If True, the bin edges will correspond to quantiles of `times` (default).\n",
    "        Otherwise, generates equally-spaced bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of bin edges.\n",
    "    \"\"\"\n",
    "    if num_bins is None:\n",
    "        num_bins = math.ceil(math.sqrt(len(times)))\n",
    "    if use_quantiles:\n",
    "        bins = np.unique(np.quantile(times, np.linspace(0, 1, num_bins)))\n",
    "    else:\n",
    "        bins = np.linspace(times.min(), times.max(), num_bins)\n",
    "    return bins\n",
    "\n",
    "# utility functions\n",
    "def encode_survival(time, event, bins):\n",
    "    \"\"\"Encodes survival time and event indicator in the format\n",
    "    required for MTLR training.\n",
    "    \n",
    "    For uncensored instances, one-hot encoding of binned survival time\n",
    "    is generated. Censoring is handled differently, with all possible\n",
    "    values for event time encoded as 1s. For example, if 5 time bins are used,\n",
    "    an instance experiencing event in bin 3 is encoded as [0, 0, 0, 1, 0], and \n",
    "    instance censored in bin 2 as [0, 0, 1, 1, 1]. Note that an additional\n",
    "    'catch-all' bin is added, spanning the range `(bins.max(), inf)`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.ndarray\n",
    "        Array of event or censoring times.\n",
    "    event : np.ndarray\n",
    "        Array of event indicators (0 = censored).\n",
    "    bins : np.ndarray\n",
    "        Bins used for time axis discretisation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Encoded survival times.\n",
    "    \"\"\"\n",
    "    time = np.clip(time, 0, bins.max())\n",
    "    bin_idxs = np.digitize(time, bins)\n",
    "    # add extra bin [max_time, inf) at the end\n",
    "    y = np.zeros((time.shape[0], bins.shape[0] + 1))#\n",
    "    for i, e in enumerate(event):\n",
    "        bin_idx = bin_idxs[i]\n",
    "        if e == 1:\n",
    "            y[i, bin_idx] = 1\n",
    "        else:\n",
    "            y[i, bin_idx:] = 1\n",
    "    return torch.tensor(y, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc45a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCGA_csv_preprocess(df, cols_required, cols_to_one_hot,time_col, event_col):\n",
    "    '''\n",
    "    df : pd.DataFrame\n",
    "    cols_required : List\n",
    "    cols_to_one_hot : List\n",
    "    '''\n",
    "    df = df[cols_required]\n",
    "    df = df.rename(columns={time_col :\"time\"})\n",
    "    df = df.rename(columns={event_col :\"event\"})\n",
    "    \n",
    "    for col in cols_to_one_hot:\n",
    "        one_hot_encoded = pd.get_dummies(df[col])\n",
    "        df = pd.concat([df,one_hot_encoded],axis=1)\n",
    "    # cols_to_one_hot.remove('histological_type')\n",
    "    df = df.drop(cols_to_one_hot, axis = 1)\n",
    "    df.dropna(inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412e51d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>event</th>\n",
       "      <th>time</th>\n",
       "      <th>Stage I</th>\n",
       "      <th>Stage IA</th>\n",
       "      <th>Stage IB</th>\n",
       "      <th>Stage II</th>\n",
       "      <th>Stage IIA</th>\n",
       "      <th>Stage IIB</th>\n",
       "      <th>Stage III</th>\n",
       "      <th>Stage IIIA</th>\n",
       "      <th>Stage IIIB</th>\n",
       "      <th>Stage IIIC</th>\n",
       "      <th>Stage IV</th>\n",
       "      <th>Stage V</th>\n",
       "      <th>[Not Available]</th>\n",
       "      <th>Infiltrating Ductal Carcinoma</th>\n",
       "      <th>Infiltrating Lobular Carcinoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>TCGA-3C-AAAU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>TCGA-3C-AALI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4005.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>TCGA-3C-AALJ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>TCGA-3C-AALK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>TCGA-4H-AAAK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>TCGA-WT-AB44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>TCGA-XX-A899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>TCGA-XX-A89A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>TCGA-Z7-A8R5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3287.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>TCGA-Z7-A8R6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3256.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient_id  event    time  Stage I  Stage IA  Stage IB  Stage II  \\\n",
       "496   TCGA-3C-AAAU    0.0  4047.0        0         0         0         0   \n",
       "497   TCGA-3C-AALI    0.0  4005.0        0         0         0         0   \n",
       "498   TCGA-3C-AALJ    0.0  1474.0        0         0         0         0   \n",
       "499   TCGA-3C-AALK    0.0  1448.0        0         1         0         0   \n",
       "500   TCGA-4H-AAAK    0.0   348.0        0         0         0         0   \n",
       "...            ...    ...     ...      ...       ...       ...       ...   \n",
       "1478  TCGA-WT-AB44    0.0   883.0        0         1         0         0   \n",
       "1479  TCGA-XX-A899    0.0   467.0        0         0         0         0   \n",
       "1480  TCGA-XX-A89A    0.0   488.0        0         0         0         0   \n",
       "1481  TCGA-Z7-A8R5    0.0  3287.0        0         0         0         0   \n",
       "1482  TCGA-Z7-A8R6    0.0  3256.0        1         0         0         0   \n",
       "\n",
       "      Stage IIA  Stage IIB  Stage III  Stage IIIA  Stage IIIB  Stage IIIC  \\\n",
       "496           0          0          0           0           0           0   \n",
       "497           0          1          0           0           0           0   \n",
       "498           0          1          0           0           0           0   \n",
       "499           0          0          0           0           0           0   \n",
       "500           0          0          0           1           0           0   \n",
       "...         ...        ...        ...         ...         ...         ...   \n",
       "1478          0          0          0           0           0           0   \n",
       "1479          0          0          0           1           0           0   \n",
       "1480          0          1          0           0           0           0   \n",
       "1481          0          0          0           1           0           0   \n",
       "1482          0          0          0           0           0           0   \n",
       "\n",
       "      Stage IV  Stage V  [Not Available]  Infiltrating Ductal Carcinoma  \\\n",
       "496          0        1                0                              0   \n",
       "497          0        0                0                              1   \n",
       "498          0        0                0                              1   \n",
       "499          0        0                0                              1   \n",
       "500          0        0                0                              0   \n",
       "...        ...      ...              ...                            ...   \n",
       "1478         0        0                0                              0   \n",
       "1479         0        0                0                              0   \n",
       "1480         0        0                0                              0   \n",
       "1481         0        0                0                              0   \n",
       "1482         0        0                0                              0   \n",
       "\n",
       "      Infiltrating Lobular Carcinoma  \n",
       "496                                1  \n",
       "497                                0  \n",
       "498                                0  \n",
       "499                                0  \n",
       "500                                1  \n",
       "...                              ...  \n",
       "1478                               1  \n",
       "1479                               1  \n",
       "1480                               1  \n",
       "1481                               1  \n",
       "1482                               1  \n",
       "\n",
       "[987 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcga_path = \"/workspace/tcga_clinical_stage_combined.csv\" \n",
    "tcga_data = pd.read_csv(tcga_path)\n",
    "\n",
    "cols_required = ['patient_id', 'OS','OS.time', 'ajcc_pathologic_tumor_stage', 'histological_type']\n",
    "cols_to_one_hot = ['ajcc_pathologic_tumor_stage', 'histological_type']\n",
    "time_col = 'OS.time'\n",
    "event_col = 'OS' \n",
    "tcga_data.rename(columns = {'Patient ID' : 'patient_id'}, inplace = True)\n",
    "tcga_data['histological_type'] = tcga_data['histological_type'].fillna('SKCM')\n",
    "tcga_data = tcga_data[tcga_data['Organ']=='Breast']\n",
    "patient_data = TCGA_csv_preprocess(tcga_data, cols_required, cols_to_one_hot, time_col, event_col)\n",
    "print(len(patient_data.columns))\n",
    "patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01d7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #alive\n",
    "with open(\"genomic_all_patients_all.pkl\", 'rb') as f:\n",
    "    final= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13055bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n",
      "Number of rows before dropping nan :  986\n",
      "Number of rows after dropping nan :  986\n",
      "ENSG00000182162.11_PAR_Y removed\n",
      "ENSG00000205755.12_PAR_Y removed\n",
      "ENSG00000280757.1 removed\n",
      "Number of rows after dropping some genes :  986\n"
     ]
    }
   ],
   "source": [
    "#Merging clinical and genomic data\n",
    "df = pd.merge(final,patient_data,on='patient_id')\n",
    "print(df['patient_id'].nunique())\n",
    "print(\"Number of rows before dropping nan : \",len(df))\n",
    "df = df.dropna()\n",
    "print(\"Number of rows after dropping nan : \",len(df))\n",
    "cols = df.columns\n",
    "cols_not_dropped = ['patient_id', 'Organ', 'type', 'ajcc_pathologic_tumor_stage', 'histological_type', 'drug_name','SKCM']\n",
    "for col in cols:\n",
    "    if col in cols_not_dropped:\n",
    "        continue\n",
    "    if((col=='event') or (col=='patient_id') or col=='[Discrepancy]' or col =='[Not Available]'):\n",
    "        continue\n",
    " \n",
    "#     if(df[col]!=0).any:\n",
    "#         continue\n",
    "    if(df[col].sum()!=0):\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{col} removed\")\n",
    "        df = df.drop(col,axis =1)\n",
    "# # df = df.drop(\"patient_id\",axis=1)\n",
    "print(\"Number of rows after dropping some genes : \",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7a4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214b3784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000002834.18</th>\n",
       "      <th>ENSG00000005073.6</th>\n",
       "      <th>ENSG00000005339.15</th>\n",
       "      <th>ENSG00000006468.14</th>\n",
       "      <th>ENSG00000007237.19</th>\n",
       "      <th>ENSG00000007312.13</th>\n",
       "      <th>ENSG00000009709.12</th>\n",
       "      <th>ENSG00000010671.16</th>\n",
       "      <th>ENSG00000012048.23</th>\n",
       "      <th>ENSG00000015285.11</th>\n",
       "      <th>...</th>\n",
       "      <th>Stage IIB</th>\n",
       "      <th>Stage III</th>\n",
       "      <th>Stage IIIA</th>\n",
       "      <th>Stage IIIB</th>\n",
       "      <th>Stage IIIC</th>\n",
       "      <th>Stage IV</th>\n",
       "      <th>Stage V</th>\n",
       "      <th>[Not Available]</th>\n",
       "      <th>Infiltrating Ductal Carcinoma</th>\n",
       "      <th>Infiltrating Lobular Carcinoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.633187</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>9.322329</td>\n",
       "      <td>7.731931</td>\n",
       "      <td>8.197814</td>\n",
       "      <td>4.75359</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.054439</td>\n",
       "      <td>6.418365</td>\n",
       "      <td>6.335054</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.368198</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>9.645429</td>\n",
       "      <td>8.246958</td>\n",
       "      <td>8.832442</td>\n",
       "      <td>5.402677</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.66296</td>\n",
       "      <td>6.660575</td>\n",
       "      <td>6.037871</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.773539</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>9.431402</td>\n",
       "      <td>8.561019</td>\n",
       "      <td>8.371474</td>\n",
       "      <td>4.077537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.102559</td>\n",
       "      <td>7.160069</td>\n",
       "      <td>5.529429</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.453287</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>9.105313</td>\n",
       "      <td>7.404888</td>\n",
       "      <td>8.50025</td>\n",
       "      <td>4.521789</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>5.913503</td>\n",
       "      <td>6.308098</td>\n",
       "      <td>6.177944</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.35738</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>8.65904</td>\n",
       "      <td>6.529419</td>\n",
       "      <td>6.837333</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>7.134891</td>\n",
       "      <td>5.247024</td>\n",
       "      <td>6.663133</td>\n",
       "      <td>4.75359</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>9.43875</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>9.031572</td>\n",
       "      <td>5.932245</td>\n",
       "      <td>6.385194</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>6.517671</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>10.564215</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>9.220489</td>\n",
       "      <td>6.104793</td>\n",
       "      <td>7.81682</td>\n",
       "      <td>4.882802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.473891</td>\n",
       "      <td>8.993552</td>\n",
       "      <td>6.590301</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>9.178643</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>8.806724</td>\n",
       "      <td>5.686975</td>\n",
       "      <td>6.732211</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.298317</td>\n",
       "      <td>6.995766</td>\n",
       "      <td>5.241747</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>11.154606</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>9.123256</td>\n",
       "      <td>7.231287</td>\n",
       "      <td>8.224432</td>\n",
       "      <td>5.402677</td>\n",
       "      <td>6.579251</td>\n",
       "      <td>6.222576</td>\n",
       "      <td>8.233503</td>\n",
       "      <td>6.408529</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>9.739791</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>8.902728</td>\n",
       "      <td>6.568078</td>\n",
       "      <td>8.276903</td>\n",
       "      <td>6.095825</td>\n",
       "      <td>6.287859</td>\n",
       "      <td>7.415777</td>\n",
       "      <td>6.742881</td>\n",
       "      <td>7.436617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows Ã— 763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ENSG00000002834.18 ENSG00000005073.6 ENSG00000005339.15  \\\n",
       "0             9.633187          2.944439           9.322329   \n",
       "1             9.368198          3.258097           9.645429   \n",
       "2             8.773539          2.197225           9.431402   \n",
       "3             9.453287          2.944439           9.105313   \n",
       "4              9.35738          3.218876            8.65904   \n",
       "..                 ...               ...                ...   \n",
       "981            9.43875          3.295837           9.031572   \n",
       "982          10.564215          2.302585           9.220489   \n",
       "983           9.178643          2.197225           8.806724   \n",
       "984          11.154606          3.713572           9.123256   \n",
       "985           9.739791          3.806662           8.902728   \n",
       "\n",
       "    ENSG00000006468.14 ENSG00000007237.19 ENSG00000007312.13  \\\n",
       "0             7.731931           8.197814            4.75359   \n",
       "1             8.246958           8.832442           5.402677   \n",
       "2             8.561019           8.371474           4.077537   \n",
       "3             7.404888            8.50025           4.521789   \n",
       "4             6.529419           6.837333           3.258097   \n",
       "..                 ...                ...                ...   \n",
       "981           5.932245           6.385194           2.197225   \n",
       "982           6.104793            7.81682           4.882802   \n",
       "983           5.686975           6.732211           3.135494   \n",
       "984           7.231287           8.224432           5.402677   \n",
       "985           6.568078           8.276903           6.095825   \n",
       "\n",
       "    ENSG00000009709.12 ENSG00000010671.16 ENSG00000012048.23  \\\n",
       "0             0.693147           6.054439           6.418365   \n",
       "1             0.693147            5.66296           6.660575   \n",
       "2                  0.0           6.102559           7.160069   \n",
       "3             1.791759           5.913503           6.308098   \n",
       "4             7.134891           5.247024           6.663133   \n",
       "..                 ...                ...                ...   \n",
       "981            1.94591           5.181784           6.517671   \n",
       "982                0.0           6.473891           8.993552   \n",
       "983                0.0           5.298317           6.995766   \n",
       "984           6.579251           6.222576           8.233503   \n",
       "985           6.287859           7.415777           6.742881   \n",
       "\n",
       "    ENSG00000015285.11  ... Stage IIB Stage III Stage IIIA Stage IIIB  \\\n",
       "0             6.335054  ...         0         0          0          0   \n",
       "1             6.037871  ...         1         0          0          0   \n",
       "2             5.529429  ...         1         0          0          0   \n",
       "3             6.177944  ...         0         0          1          0   \n",
       "4              4.75359  ...         0         0          0          0   \n",
       "..                 ...  ...       ...       ...        ...        ...   \n",
       "981           5.192957  ...         0         0          1          0   \n",
       "982           6.590301  ...         0         0          0          0   \n",
       "983           5.241747  ...         1         0          0          0   \n",
       "984           6.408529  ...         1         0          0          0   \n",
       "985           7.436617  ...         0         0          1          0   \n",
       "\n",
       "    Stage IIIC Stage IV Stage V [Not Available] Infiltrating Ductal Carcinoma  \\\n",
       "0            0        0       0               0                             1   \n",
       "1            0        0       0               0                             1   \n",
       "2            0        0       0               0                             1   \n",
       "3            0        0       0               0                             1   \n",
       "4            0        0       1               0                             1   \n",
       "..         ...      ...     ...             ...                           ...   \n",
       "981          0        0       0               0                             1   \n",
       "982          1        0       0               0                             1   \n",
       "983          0        0       0               0                             1   \n",
       "984          0        0       0               0                             0   \n",
       "985          0        0       0               0                             1   \n",
       "\n",
       "    Infiltrating Lobular Carcinoma  \n",
       "0                                0  \n",
       "1                                0  \n",
       "2                                0  \n",
       "3                                0  \n",
       "4                                0  \n",
       "..                             ...  \n",
       "981                              0  \n",
       "982                              0  \n",
       "983                              0  \n",
       "984                              1  \n",
       "985                              0  \n",
       "\n",
       "[986 rows x 763 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "path = \"/workspace/TCGA_BRCA2_clam_feat/pt_files/*.pt\"\n",
    "patient_feat = glob.glob(path)\n",
    "# patient_feat.remove('/workspace/TCGA_LUAD_clam_feat/pt_files/TCGA-68-8251-01Z-00-DX1.9d453881-d5b3-4527-93fa-799f34ba2c8f.pt')\n",
    "patient_ids = [path.split('/')[-1][:12] for path in patient_feat]\n",
    "# # Clinical Data\n",
    "# tcga_path = \"/workspace/tcga_clinical_stage_combined.csv\" \n",
    "# tcga_data = pd.read_csv(tcga_path)\n",
    "# tcga_data = tcga_data[tcga_data['Organ']=='Breast']\n",
    "# cols_req = ['Patient ID', 'OS','OS.time', 'ajcc_pathologic_tumor_stage', 'histological_type']\n",
    "# tcga_data = tcga_data[cols_req]\n",
    "# tcga_data = pd.get_dummies(tcga_data, columns=['ajcc_pathologic_tumor_stage'])\n",
    "# tcga_data = pd.get_dummies(tcga_data, columns=['histological_type'])\n",
    "\n",
    "# tcga_data = tcga_data.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c096d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/workspace/Genomic_image/CMTA'\n",
      "/workspace/Genomic_image\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/Genomic_image/CMTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f05d4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_0 = pd.read_csv('/workspace/CMTA/splits/5foldcv/tcga_brca/splits_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c8b007f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986, 697, 764)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[df['patient_id'].isin(list(split_0['train']))] \n",
    "len(df), len(df_train), len(split_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e4c64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TCGA-3C-AALI</td>\n",
       "      <td>TCGA-5T-A9QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TCGA-3C-AALJ</td>\n",
       "      <td>TCGA-A1-A0SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TCGA-3C-AALK</td>\n",
       "      <td>TCGA-A1-A0SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TCGA-4H-AAAK</td>\n",
       "      <td>TCGA-A1-A0SI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TCGA-5L-AAT0</td>\n",
       "      <td>TCGA-A1-A0SJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>759</td>\n",
       "      <td>TCGA-WT-AB41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>760</td>\n",
       "      <td>TCGA-WT-AB44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>761</td>\n",
       "      <td>TCGA-XX-A89A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>762</td>\n",
       "      <td>TCGA-Z7-A8R5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>763</td>\n",
       "      <td>TCGA-Z7-A8R6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         train           val\n",
       "0             0  TCGA-3C-AALI  TCGA-5T-A9QA\n",
       "1             1  TCGA-3C-AALJ  TCGA-A1-A0SB\n",
       "2             2  TCGA-3C-AALK  TCGA-A1-A0SE\n",
       "3             3  TCGA-4H-AAAK  TCGA-A1-A0SI\n",
       "4             4  TCGA-5L-AAT0  TCGA-A1-A0SJ\n",
       "..          ...           ...           ...\n",
       "759         759  TCGA-WT-AB41           NaN\n",
       "760         760  TCGA-WT-AB44           NaN\n",
       "761         761  TCGA-XX-A89A           NaN\n",
       "762         762  TCGA-Z7-A8R5           NaN\n",
       "763         763  TCGA-Z7-A8R6           NaN\n",
       "\n",
       "[764 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12bd412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty dictionary to store patient IDs and corresponding times\n",
    "patient_times = []\n",
    "clinical_data = []\n",
    "count = 0\n",
    "# Iterate over each patient ID\n",
    "times = []\n",
    "events = []\n",
    "ids = []\n",
    "for i in range(len(patient_ids)):\n",
    "    patient_id = patient_ids[i]\n",
    "    # Find the index of the row where the patient ID matches\n",
    "    idx = df.index[df['patient_id'] == patient_id]\n",
    "    # Check if any row is found\n",
    "    if len(idx) > 0:\n",
    "        # Retrieve the time corresponding to the patient ID\n",
    "        df_ = df[df['patient_id'] == patient_id]\n",
    "        time = df_['time'].values[0]\n",
    "        event = df_['event'].values[0]\n",
    "        \n",
    "        # Store the patient ID and corresponding time in the dictionary\n",
    "        patient_times.append((patient_id, time, event))    \n",
    "        times.append(time)\n",
    "        events.append(event)\n",
    "        ids.append(patient_id)\n",
    "        df_ = df_.drop(['patient_id', 'time', 'event'], axis=1)\n",
    "        c = df_.values[0]\n",
    "        clinical_data.append(c)\n",
    "    else:\n",
    "        del patient_feat[i-count]\n",
    "        count = count + 1\n",
    "\n",
    "# surv_data = patient_times\n",
    "\n",
    "time_bins = make_time_bins(times)\n",
    "num_time_bins = len(time_bins)+1\n",
    "surv_time_bins = encode_survival(times, events, time_bins)\n",
    "print(num_time_bins)\n",
    "\n",
    "surv_data = [(ids[i], surv_time_bins[i], times[i], events[i]) for i in range(len(times))]\n",
    "\n",
    "# Combine wsi_paths, annot_paths, and surv_data into a single list of tuples\n",
    "data = list(zip(patient_feat, surv_data, clinical_data))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data =  train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "# Unzip the train and test data back into separate lists\n",
    "train_feat_paths,  train_surv_data, train_clini = zip(*train_data)\n",
    "test_feat_paths, test_surv_data, test_clini= zip(*test_data)\n",
    "val_feat_paths, val_surv_data, val_clini = zip(*val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "014f3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "# ('/workspace/TCGA_BRCA_clam_feat/pt_files/TCGA-A7-A2KD-01A-03-TSC.CC1E46B0-7920-4F91-AE61-A9A78BF724DB.pt',\n",
    "#   ('TCGA-A7-A2KD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7dc1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_surv_data\n",
    "# train_data\n",
    "# len(data) # = 522 (luad) 891 (brca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edbc4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13a7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(data_loader, model, simLoss, epoch, mtlr = True):\n",
    "    time_tensor = torch.tensor([])\n",
    "    event_tensor = torch.tensor([])\n",
    "    pred_tensor = torch.tensor([])\n",
    "    risk_tensor = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        SIM_LOSS_LATENT = 0.0\n",
    "        SIM_LOSS_GENOMIC = 0.0\n",
    "        COX_LOSS = 0.0\n",
    "        MTLR_LOSS = 0.0\n",
    "        for x, time, event, clinical_data, time_val in data_loader:\n",
    "            x, event, time = x.to(device), event.to(device), time.to(device)\n",
    "            clinical_data = clinical_data.to(device)\n",
    "            time_val = time_val.to(device)\n",
    "            \n",
    "            n_dead = event.sum()\n",
    "            if n_dead == 0:\n",
    "                continue\n",
    "        \n",
    "            out = model(data = x, clinical = clinical_data)\n",
    "            \n",
    "            # cox loss\n",
    "            coxloss = CoxLoss(out['risk'], event, time_val)\n",
    "            \n",
    "            mtlr_loss = 0.0\n",
    "            # mtlr loss \n",
    "            if mtlr:\n",
    "                mtlr_loss = mtlr_neg_log_likelihood(out['logits'], time, average=True)\n",
    "            \n",
    "            #similarity_loss\n",
    "            sim_loss_latent = simLoss(out['genomic_latent'], out['wsi_latent'])\n",
    "            sim_loss_genomic = simLoss(out['genomic_latent'], out['genomics_from_pathomics'])\n",
    "    \n",
    "            loss = coxloss + mtlr_loss+sim_loss_genomic+sim_loss_latent\n",
    "        \n",
    "            running_loss = loss.item() + running_loss\n",
    "            \n",
    "            time_tensor = torch.cat((time_tensor, time_val.cpu()), dim=0)\n",
    "            event_tensor = torch.cat((event_tensor, event.cpu()), dim=0)\n",
    "            pred_tensor = torch.cat((pred_tensor, out['logits'].cpu()), dim=0)\n",
    "            risk_tensor = torch.cat((risk_tensor, out['risk'].cpu()), dim=0)\n",
    "            \n",
    "            # cin = concordance_index(y_time.cpu(), -1*y_pred.cpu(), y_event.cpu())\n",
    "            SIM_LOSS_LATENT = SIM_LOSS_LATENT + sim_loss_latent.item()\n",
    "            SIM_LOSS_GENOMIC = SIM_LOSS_GENOMIC + sim_loss_genomic.item()\n",
    "            COX_LOSS = COX_LOSS + coxloss.item()\n",
    "            MTLR_LOSS = MTLR_LOSS + mtlr_loss.item()\n",
    "            \n",
    "        running_loss = running_loss/len(data_loader)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/val\", running_loss, epoch)\n",
    "        writer.add_scalar(\"SIM_LOSS_LATENT/val\", SIM_LOSS_LATENT, epoch)\n",
    "        writer.add_scalar(\"SIM_LOSS_GENOMIC/val\", SIM_LOSS_GENOMIC, epoch)\n",
    "        writer.add_scalar(\"COX_LOSS/val\", COX_LOSS, epoch)\n",
    "        writer.add_scalar(\"MTLR_LOSS/val\", MTLR_LOSS, epoch)\n",
    "        \n",
    "        y_time, y_event, y_pred =   time_tensor, event_tensor, pred_tensor \n",
    "        if mtlr:\n",
    "            pred_risk = mtlr_risk(y_pred).cpu().numpy()\n",
    "            time_sorted, indices_times = torch.sort(y_time)\n",
    "            risk_tensor = risk_tensor[indices_times]\n",
    "            event_ = y_event[indices_times]\n",
    "              \n",
    "            cin = concordance_index(time_sorted, -1*risk_tensor, event_)\n",
    "            print(cin)\n",
    "            writer.add_scalar(\"Cindex_Cox/val\", cin, epoch)\n",
    "            cin = concordance_index(time_tensor, -pred_risk, event_observed=event_tensor)\n",
    "            writer.add_scalar(\"Cindex_MTLR/val\", cin, epoch)\n",
    "        else:\n",
    "            cin = concordance_index(y_time, -1*y_pred, y_event)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return running_loss, cin\n",
    "\n",
    "def train_model(model, train_loader, val_loader,\n",
    "                num_epochs = 1000, lr = 0.01, weight_decay = 0.,\n",
    "                l1_reg = 0, batch_size = 10,\n",
    "                device=\"cuda:0\", l2_reg = 0.0, mtlr = True):\n",
    "    # array conraining a list of losses\n",
    "    \n",
    "    loss_train = [] \n",
    "    loss_val = [] \n",
    "    old_cin = 0\n",
    "    prev_loss = 10000\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    simLoss = nn.MSELoss()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    final_model = model \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        SIM_LOSS_LATENT = 0.0\n",
    "        SIM_LOSS_GENOMIC = 0.0\n",
    "        COX_LOSS = 0.0\n",
    "        MTLR_LOSS = 0.0\n",
    "        for x, time, event, clinical_data, time_val in train_loader:\n",
    "            \n",
    "            x, event, time = x.to(device), event.to(device), time.to(device)\n",
    "            clinical_data = clinical_data.to(device)\n",
    "            time_val = time_val.to(device)\n",
    "            \n",
    "            n_dead = event.sum()\n",
    "            if n_dead == 0:\n",
    "                continue\n",
    "            if (torch.isnan(x).any() or torch.isnan(event).any() or torch.isnan(time).any()):\n",
    "                continue\n",
    "            \n",
    "            out = model(data = x, clinical = clinical_data)\n",
    "        \n",
    "            # cox loss\n",
    "            coxloss = CoxLoss(out['risk'], event, time_val)\n",
    "            \n",
    "            mtlr_loss = 0.0\n",
    "            # mtlr loss \n",
    "            if mtlr:\n",
    "                mtlr_loss = mtlr_neg_log_likelihood(out['logits'], time, average=True)\n",
    "            \n",
    "            #similarity_loss\n",
    "            \n",
    "            sim_loss_latent = simLoss(out['genomic_latent'], out['wsi_latent'])\n",
    "            \n",
    "            sim_loss_genomic = simLoss(out['genomic_latent'], out['genomics_from_pathomics'])\n",
    "    \n",
    "            loss = coxloss + mtlr_loss+sim_loss_genomic+sim_loss_latent\n",
    "            \n",
    "#             # Add L1 regularization to the the first layer and compute loss\n",
    "#             l1_loss = torch.tensor(0.).cuda()\n",
    "#             for param in model.parameters():\n",
    "#                 l1_loss += torch.norm(param, 1)\n",
    "#                 break\n",
    "#             loss = loss + l1_loss * l1_reg\n",
    "\n",
    "            SIM_LOSS_LATENT = SIM_LOSS_LATENT + sim_loss_latent.item()\n",
    "            SIM_LOSS_GENOMIC = SIM_LOSS_GENOMIC + sim_loss_genomic.item()\n",
    "            COX_LOSS = COX_LOSS + coxloss.item()\n",
    "            MTLR_LOSS = MTLR_LOSS + mtlr_loss.item()\n",
    "            \n",
    "            running_loss = running_loss + loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        running_loss = running_loss/len(train_loader)  \n",
    "        SIM_LOSS_LATENT = SIM_LOSS_LATENT/len(train_loader)\n",
    "        SIM_LOSS_GENOMIC = SIM_LOSS_GENOMIC/len(train_loader)\n",
    "        COX_LOSS = COX_LOSS/len(train_loader)\n",
    "        MTLR_LOSS = MTLR_LOSS/len(train_loader)\n",
    "        \n",
    "        loss_train.append(running_loss)\n",
    "        print(running_loss)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
    "        writer.add_scalar(\"SIM_LOSS_LATENT/train\", SIM_LOSS_LATENT, epoch)\n",
    "        writer.add_scalar(\"SIM_LOSS_GENOMIC/train\", SIM_LOSS_GENOMIC, epoch)\n",
    "        writer.add_scalar(\"COX_LOSS/train\", COX_LOSS, epoch)\n",
    "        writer.add_scalar(\"MTLR_LOSS/train\", MTLR_LOSS, epoch)\n",
    "        writer.flush()\n",
    "        # tr, c = test_model(train_loader, model)\n",
    "        # print(\"Train Data\", c)\n",
    "        val_loss, cin = test_model(val_loader, model, simLoss, epoch)\n",
    "        loss_val.append(val_loss)\n",
    "        if(prev_loss>val_loss):\n",
    "            final_model = model\n",
    "            torch.save(model.state_dict(), 'exp1.p')\n",
    "            prev_loss = val_loss\n",
    "            \n",
    "        print(\"Val Data\", cin)\n",
    "    final_model.eval()\n",
    "    return final_model, loss_train, loss_val\n",
    "\n",
    "\n",
    "# Define DataLoader for the SurvivalDataset\n",
    "def create_dataloader(feat_paths, surv_data, clinical_data, transform=None, batch_size=32, num_patches = 10000):\n",
    "    dataset = SurvivalDataset(feat_paths, surv_data, clinical_data, num_patches)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ae51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1bd97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoss :\\n|genomics_from_pathomics-genomic_latent| + |genomic_latent-wsi_latent| + |cox_loss| + |mtlr_loss|\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TransMIL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransMIL, self).__init__()\n",
    "        self.pos_layer = PPEG(dim=512)\n",
    "        self._fc1 = nn.Sequential(nn.Linear(1024, 512), nn.ReLU())\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 512))\n",
    "        self.layer1 = TransLayer(dim=512)\n",
    "        self.layer2 = TransLayer(dim=512)\n",
    "        self.norm = nn.LayerNorm(512)\n",
    "     \n",
    "\n",
    "    def forward(self, data, encoder =0):\n",
    "\n",
    "        h = data #[B, n, 1024]\n",
    "        \n",
    "        if encoder==1:\n",
    "            h = self._fc1(h) #[B, n, 512]\n",
    "        \n",
    "        #---->pad\n",
    "        H = h.shape[1]\n",
    "    \n",
    "        _H, _W = int(np.ceil(np.sqrt(H))), int(np.ceil(np.sqrt(H)))\n",
    "        add_length = _H * _W - H\n",
    "        h = torch.cat([h, h[:,:add_length,:]],dim = 1) #[B, N, 512]\n",
    "        \n",
    "        #---->cls_token\n",
    "        B = h.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1).cuda()\n",
    "        h = torch.cat((cls_tokens, h), dim=1)\n",
    "   \n",
    "        #---->Translayer x1\n",
    "        h, attn_scores1= self.layer1(h) #[B, N, 512]\n",
    "        \n",
    "        #---->PPEG\n",
    "        h = self.pos_layer(h, _H, _W) #[B, N, 512]\n",
    "        \n",
    "        #---->Translayer x2\n",
    "        h, attn_scores2 = self.layer2(h) #[B, N, 512]\n",
    "        \n",
    "        h = self.norm(h)\n",
    "        return h[:,0], h[:,1:] #cls_token, sequence\n",
    "        \n",
    "\n",
    "class TransSurv(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TransSurv, self).__init__()\n",
    "        self.transmil_encoder = TransMIL()\n",
    "        self.transmil_decoder = TransMIL()\n",
    "        self.n_classes = n_classes\n",
    "        self._fc2 = nn.Linear(512, 1)\n",
    "#         self._fc4 = nn.Linear(128,1)\n",
    "        self._fc_latent = nn.Linear(760, 512)\n",
    "        self.mtlr = MTLR(in_features=512, num_time_bins=self.n_classes)\n",
    "        self._fc3 = nn.Linear(256, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        h = kwargs['data'].float() #[B, n, 1024]\n",
    "        \n",
    "        h, path_encoded = self.transmil_encoder(h, encoder=1)\n",
    "        \n",
    "        wsi_latent = h\n",
    "        \n",
    "        #genomic data\n",
    "        genomic = kwargs['clinical'].float()\n",
    "        genomic_latent = self._fc_latent(genomic) #(512, 1)\n",
    "        \n",
    "        genomic_from_pathomics,_ = self.transmil_decoder(path_encoded)\n",
    "            \n",
    "\n",
    "        risk = self._fc2(genomic_latent) # (512 -> 1) cox risk\n",
    "#         risk = self._fc4(risk)\n",
    "    \n",
    "        logits = self.mtlr(h) #(512 -> 1) mtlr risk\n",
    "        \n",
    "        \n",
    "        out = {\"genomic_latent\" : genomic_latent,\n",
    "              \"wsi_latent\" : wsi_latent,\n",
    "               \"genomics_from_pathomics\" : genomic_from_pathomics,\n",
    "               \"risk\" : risk,\n",
    "               \"logits\" : logits\n",
    "              }\n",
    "        return out\n",
    "    \n",
    "\"\"\"\n",
    "Loss :\n",
    "|genomics_from_pathomics-genomic_latent| + |genomic_latent-wsi_latent| + |cox_loss| + |mtlr_loss|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8af9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data :  640\n",
      "Number of test data :  179\n",
      "Number of val data :  72\n",
      "20.556694772508408\n",
      "0.7357954545454546\n",
      "Val Data 0.3664772727272727\n",
      "8.889225588904488\n",
      "0.5852272727272727\n",
      "Val Data 0.48863636363636365\n",
      "6.27777640024821\n",
      "0.6051136363636364\n",
      "Val Data 0.4460227272727273\n",
      "4.598914517296685\n",
      "0.6846590909090909\n",
      "Val Data 0.6107954545454546\n",
      "3.7148855792151556\n",
      "0.6960227272727273\n",
      "Val Data 0.2784090909090909\n",
      "3.2241288555992975\n",
      "0.7386363636363636\n",
      "Val Data 0.4744318181818182\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir = 'runs/brca_mtlr_cox_fusion_exp3')\n",
    "\n",
    "\n",
    "print(\"Number of train data : \", len(train_feat_paths))\n",
    "print(\"Number of test data : \", len(test_feat_paths))\n",
    "print(\"Number of val data : \", len(val_feat_paths)) \n",
    "# Initialize hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create DataLoader for the SurvivalDataset\n",
    "\n",
    "train_loader = create_dataloader(train_feat_paths,  train_surv_data, train_clini, batch_size=72, num_patches=10000)\n",
    "test_loader = create_dataloader(test_feat_paths,  test_surv_data, test_clini, batch_size=64)\n",
    "val_loader = create_dataloader(val_feat_paths,  val_surv_data, val_clini, batch_size=64)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransSurv(n_classes=num_time_bins).to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "final_model, train_loss, val_loss = train_model(model, train_loader, val_loader, \n",
    "                num_epochs = 6,lr = 0.001, l1_reg = 0, batch_size = 10,\n",
    "                device= device, l2_reg = 0.0)\n",
    "\n",
    "# Evaluate the model\n",
    "# test(model, test_loader, criterion)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dc6c5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransSurv(\n",
       "  (transmil_encoder): TransMIL(\n",
       "    (pos_layer): PPEG(\n",
       "      (proj): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "      (proj1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "      (proj2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    )\n",
       "    (_fc1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (layer1): TransLayer(\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): NystromAttention(\n",
       "        (to_qkv): Linear(in_features=512, out_features=384, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (res_conv): Conv2d(2, 2, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): TransLayer(\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): NystromAttention(\n",
       "        (to_qkv): Linear(in_features=512, out_features=384, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (res_conv): Conv2d(2, 2, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transmil_decoder): TransMIL(\n",
       "    (pos_layer): PPEG(\n",
       "      (proj): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "      (proj1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
       "      (proj2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    )\n",
       "    (_fc1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (layer1): TransLayer(\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): NystromAttention(\n",
       "        (to_qkv): Linear(in_features=512, out_features=384, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (res_conv): Conv2d(2, 2, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): TransLayer(\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): NystromAttention(\n",
       "        (to_qkv): Linear(in_features=512, out_features=384, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (res_conv): Conv2d(2, 2, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (_fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (_fc_latent): Linear(in_features=760, out_features=512, bias=True)\n",
       "  (mtlr): MTLR()\n",
       "  (_fc3): Linear(in_features=256, out_features=31, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_file='brca_mtlr_cox_fusion_exp3.p'\n",
    "model.load_state_dict(torch.load(save_file))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa570777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8b908a7340>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb21f583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6254893431926924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.525951226552327, 0.5063070900391474)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(test_loader, model, simLoss=nn.MSELoss(), epoch=0)\n",
    "#0.6727 - genomic, val :0.57286, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, time, event, clinical_data, time_val in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af84fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af0ec6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "log_target = F.log_softmax(torch.rand(3, 5), dim=1)\n",
    ">>> output = kl_loss(input, log_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e3543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ea81a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "488ec844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/20240612-163257'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime, os\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f01c3530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 178758), started 0:01:53 ago. (Use '!kill 178758' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3bca867ab7b5f1d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3bca867ab7b5f1d5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dad81f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 178758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical + Genomic + WSI : Fusion Model\n",
    "# Clinical + genomic : Fusion Model\n",
    "# clinical "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
